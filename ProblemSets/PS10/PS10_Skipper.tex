\documentclass{article}
\usepackage[utf8]{inputenc}

\title{PS10}
\author{Alex Skipper }
\date{February. 2018}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}
\maketitle
Worked with Conrad 
\section{Part 7}
From tuning model\\
Tree\\
minsplit=10;\\ 
minbucket=6; \\
cp=0.0269 : \\
f1.test.mean=0.89,\\
gmean.test.mean=0.686\\



Logit  Regression\\
lambda=0.0263;\\
alpha=0.786 :\\
f1.test.mean=0.897\\
,gmean.test.mean=0.662\\



Neural Network\\
size=9; \\
decay=0.357; \\
maxit=1000 :\\
f1.test.mean=0.907\\
,gmean.test.mean=0.756\\




KKNN\\
 k=23 :\\
 f1.test.mean=0.897\\
 ,gmean.test.mean=0.747\\

 
 
 
SVM\\
cost=1; \\
gamma=0.5 \\
: f1.test.mean=0.904,\\
gmean.test.mean=0.736\\



\section{Part 8 and 9}




With the optimal tuning parameters: verifying the performance on cross validated sets:\\
\begin{center}
 \begin{tabular}{||c c c c c ||} 
 \hline
 Model & f1 & gmean & Out-of-sample f1 & Out-of-sample gmean\\ [0.5ex] 
 \hline\hline
 Tree & 0.896 & 0.658 & 0.8968421 & 0.6730932  \\ 
 \hline
 Logit & 0.897 & 0.662 &  0.8986422 &  0.6762722\\
 \hline
 Neural & 0.906 & 0.753 & 0.9094500 & 0.7675114 \\
 \hline
 KKNN & 0.896 & 0.744 & 0.8975970  & 0.7564945 \\
 \hline
 SVM & 0.906 & 0.735 & 0.9048742 & 0.7478175 \\
 \hline
 Bayes & 0.884 & 0.726 & 0.8825952 & 0.7340489\\[1ex] 
 \hline
\end{tabular}
\end{center}

\end{document}


Only the tree and logit models varied from the rest but were close to each other. The rest of the model's numbers are close to each other. It appears that either model will be beneficial. 





\end{document}}